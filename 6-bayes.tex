\documentclass{article}
\usepackage{malmacros}
\begin{document}

\section{Naive Bayes}

Naive Bayesian classifier are well suited for simple models with clear categories. They are fast and provide a probablistic prediction. It has good scalability even when the the amount features grow.

\subsection{Gaussian Naive Bayes}

Gaussian distribution also called normal distribution. This is for example a good choice for the IRIS dataset, where clustering apperant when comparing the features in the dataset. 

\subsection{Multinomial Naive Bayes}
Multinomial Naive Bayes is often used in text classification. It is used for discrete data. For example integer values that have a variance of frequency in the dataset.
When having a multinomial distribution, Multinomial Naive Bayes can be used for finding patterns in frequency.

\subsection{Choosing Naive Bayes for MNIST}
Multinomial Naive Bayes is the clear choice. The Gaussian Naive Bayes is not suitable, since the dataset does not have a Normal Distribution. Multinomial Naive Bayes will be used to train for looking for the likelyhood of the placement of each pixel. 

\begin{pyminted}
from libitmal import dataloaders as dl
mnist = dl.GetMNISTRaw()
\end{pyminted}

\begin{pyminted}
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version=1,cache=True)
\end{pyminted}
\begin{pyminted}
from sklearn.model_selection import train_test_split

X, y = mnist["data"], mnist["target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
\end{pyminted}
\begin{pyminted}
from sklearn.naive_bayes import MultinomialNB, GaussianNB

mul_clf = MultinomialNB()

mul_clf.fit(X_train,y_train)
mul_clf_score = mul_clf.score(X_test,y_test)
print("Multinominal score: ", mul_clf_score)

gau_clf = GaussianNB()

gau_clf.fit(X_train,y_train)
gau_clf_score = gau_clf.score(X_test,y_test)
print("Gaussian score: ", gau_clf_score)
\end{pyminted}
\begin{pyconsole}
Multinominal score:  0.8269142857142857
Gaussian score:  0.5554857142857142
\end{pyconsole}
\begin{pyminted}
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

y_test_pred_mul = mul_clf.predict(X_test)
y_test_pred_gau = gau_clf.predict(X_test)

print('Classification report ', mul_clf)
print(classification_report(y_test, y_test_pred_mul))
plt.matshow(confusion_matrix(y_test, y_test_pred_mul), cmap='Greys')
plt.show()

print('Classification report ', gau_clf)
print(classification_report(y_test, y_test_pred_gau))
plt.matshow(confusion_matrix(y_test, y_test_pred_gau), cmap='Greys')
plt.show()
\end{pyminted}
\begin{pyconsole}
Classification report  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
              precision    recall  f1-score   support

           0       0.93      0.90      0.91      1714
           1       0.88      0.94      0.91      1977
           2       0.88      0.82      0.85      1761
           3       0.80      0.82      0.81      1806
           4       0.83      0.73      0.78      1587
           5       0.86      0.67      0.75      1607
           6       0.87      0.92      0.90      1761
           7       0.94      0.83      0.88      1878
           8       0.65      0.76      0.71      1657
           9       0.69      0.83      0.76      1752

   micro avg       0.83      0.83      0.83     17500
   macro avg       0.83      0.82      0.82     17500
weighted avg       0.83      0.83      0.83     17500
\end{pyconsole}
\dsdfig{les6-mul}{5cm}{MultinomialNB.}{H}
\begin{pyconsole}
Classification report  GaussianNB(priors=None, var_smoothing=1e-09)
              precision    recall  f1-score   support

           0       0.68      0.91      0.78      1714
           1       0.80      0.95      0.87      1977
           2       0.85      0.31      0.45      1761
           3       0.76      0.35      0.48      1806
           4       0.84      0.12      0.22      1587
           5       0.61      0.04      0.08      1607
           6       0.63      0.95      0.76      1761
           7       0.90      0.29      0.44      1878
           8       0.29      0.59      0.39      1657
           9       0.38      0.94      0.54      1752

   micro avg       0.56      0.56      0.56     17500
   macro avg       0.67      0.55      0.50     17500
weighted avg       0.68      0.56      0.51     17500

\end{pyconsole}
\dsdfig{les6-gau}{5cm}{GaussianNB.}{H}
\end{document}